# ğŸ§  Multi-Agent Driving Simulator with PPO

> A modular, reinforcement-learning-ready simulator where multiple agents navigate a grid world with traffic lights, lane constraints, and realistic driving goals â€” all trained with a shared PPO policy.

---

## â“ What is this project?

This is a **2D grid-based driving simulator** built using `gymnasium` and `pygame`, where **multiple autonomous agents** learn to:

- Navigate roads & intersections
- Respect traffic lights
- Avoid collisions
- Reach a shared destination

Agents are trained using **Proximal Policy Optimization (PPO)** via `stable-baselines3`.  
The project demonstrates key concepts in **multi-agent reinforcement learning (MARL)**.

---

## ğŸš— Features

âœ… Multi-agent control with shared PPO policy  
ğŸŸ© Traffic light logic (red/green cycling)  
ğŸ“‰ Success & collision rate tracking per episode  
ğŸ”„ Reward shaping: lane-following, progress, red-light penalty, collisions  
ğŸ“Š CSV logging + automatic training visualization  
ğŸ§  Built on Stable-Baselines3 + custom `gymnasium` environment  

---

## ğŸ§© Environment Overview

- Grid-based 10x10 map  
- Lane = 1, Intersection = 2  
- Discrete action space  
- Agent observations: `[x, y, speed]`  
- Rewards based on:
  - âœ… Staying on valid lanes
  - âœ… Obeying traffic lights
  - âœ… Progress toward goal
  - âœ… Avoiding collisions
  - âœ… Goal completion bonus

---

## ğŸ¤” Why this project?

ğŸ”¬ To research **shared-policy reinforcement learning** in a simplified urban driving context

ğŸ“ To explore MARL concepts like:
- âœ… Reward shaping
- âœ… Vectorized environments
- âœ… Logging & evaluation metrics

ğŸ› ï¸ To build a **scalable & modular base** for future driving experiments:
- Decentralized control  
- Multi-goal learning  
- Curriculum learning  
- Inter-agent communication  

---

## ğŸ§± Project Structure

multi-agent-driving-simulator/</br>
â”œâ”€â”€ envs/</br>
â”‚   â”œâ”€â”€ multi_agent_env.py # Custom Gym environment</br>
â”œâ”€â”€ algorithms/</br> 
â”‚   â”œâ”€â”€train_ppo.py # PPO training loop</br> 
â”œâ”€â”€ configs/</br>
â”‚   â”œâ”€â”€ppo_config.yaml# YAML hyperparameters</br>
â”œâ”€â”€ logs/ # Training CSV logs</br> 
â”œâ”€â”€ assets/</br>
â”‚   â”œâ”€â”€10_agents.png</br>
â”‚   â”œâ”€â”€Flowchart.pdf # Visuals & flowcharts</br>
â”œâ”€â”€ main.py # Entrypoint</br> 
â”œâ”€â”€ requirements.txt</br> 
â””â”€â”€ README.md</br>

---

# ğŸ§­ Execution Flow

This section explains how your code components interact when training starts.</br>
main.py  
â”‚  
â”œâ”€â”€ calls â†’ train() from algorithms/train_ppo.py  
â”‚   â”‚  
â”‚   â”œâ”€â”€ calls â†’ make_env()  
â”‚   â”‚   â””â”€â”€ returns â†’ Monitor-wrapped MultiAgentDrivingEnv from envs/multi_agent_env.py  
â”‚   â”‚  
â”‚   â”œâ”€â”€ Initializes PPO agent (Stable-Baselines3)  
â”‚   â”œâ”€â”€ Starts training loop for total_episodes  
â”‚   â”‚  
â”‚   â”œâ”€â”€ For each episode:  
â”‚   â”‚   â”œâ”€â”€ Interacts with environment using PPO policy  
â”‚   â”‚   â”œâ”€â”€ Collects reward, collisions, success info  
â”‚   â”‚   â””â”€â”€ Logs to â†’ logs/training_log_1.csv  
â”‚   â”‚  
â”‚   â””â”€â”€ After training:  
â”‚       â””â”€â”€ Saves reward/success plot to â†’ assets/training_plot.png

---

# ğŸš€ How to Run the Code

This guide explains how to set up and run the multi-agent driving simulator with PPO training.

---

### âœ… Prerequisites

Make sure you have:

- Python 3.8+
- `pip` installed
- `git` installed (for cloning the repo)

---

### ğŸ“¦ Step 1: Clone the Repository

</pre>```bash
git clone https://github.com/aayushmistry/Multi-Agent-Self-Play-in-a-2D-Driving-Environment.git
```</pre>


### ğŸ“¥ Step 2: Install dependencies

</pre>```pip install -r requirements.txt```</pre>


### âš™ï¸ Step 3: Run training

</pre>```python main.py```</pre>

---

# ğŸ“Š Sample Output

After training, youâ€™ll get:

- ğŸ—‚ï¸ CSV log: `logs/training_log_1.csv`  
  Contains episode-wise metrics like total reward, success rate, and collisions.

- ğŸ“ˆ Plot: `assets/training_plot.png`  
  A graph showing reward improvement and agent success rate over time.

---

### ğŸ“ˆ Training Curve Example

> This plot shows the smoothed total reward and success rate during PPO training with 10 agents.

[ğŸ”— View Training_Curve](results/10_agents/10_agents.png)


---

### ğŸ§¾ Sample CSV Output (`training_log_1.csv`)

| Episode | TotalReward | SuccessCount | SuccessRate | CollisionCount | CollisionRate |
|---------|-------------|--------------|--------------|----------------|----------------|
| 0       | 15.25       | 2            | 0.20         | 1              | 0.10           |
| 1       | 22.75       | 4            | 0.40         | 0              | 0.00           |
| ...     | ...         | ...          | ...          | ...            | ...            |
| 499     | 87.95       | 7            | 0.70         | 2              | 0.20           |

---

## ğŸ“Š Experiment Showcase: 50 vs 100 Agents (Without Scripts)

As part of scalability testing, PPO training was also run using **50** and **100** agents.  
These results are uploaded here **only as performance visualizations**, and **training scripts are intentionally not included** for these experiments.

---

| Agents     | Smoothed Reward | Success Rate | Collision Rate | Notes                              |
|------------|------------------|---------------|----------------|-------------------------------------|
| 50 Agents  | âš ï¸ Unstable     | ğŸ”¸ Very Low   | ğŸ”¼ Medium       | Agents begin interfering frequently |
| 100 Agents | âŒ Volatile     | ğŸ”´ Near Zero  | ğŸ”º Extremely High | Policy collapses under crowding    |

---

### ğŸ“‰ PPO Reward Progress (Visualization Only)

#### ğŸ”¹ 50 Agents  
[ğŸ”— View 50-Agent Plot](results/50_agents/50_agents.png)


#### ğŸ”¹ 100 Agents  
[ğŸ”— View 100-Agent Plot](results/100_agents/100_agents.png)


---

ğŸ“ **Note:**  
These experiments were conducted to analyze multi-agent self-play behavior under increasing population sizes.  
They help highlight challenges in coordination, reward sparsity, and PPOâ€™s generalization limits.

ğŸ“‚ Only results are shown. No code/scripts are provided for these specific runs to keep the repository focused on the base setup.


## âœ… These metrics are useful for analyzing:
- Agent learning progress
- Generalization with different agent counts (10, 50, 100)
- Policy effectiveness and reward shaping

---

## ğŸ“„ Documentation

- [ğŸš§ Challenges & Bottleneck Analysis](CHALLENGES.md)


## ğŸ”­ Future Enhancements

ğŸ§© Model checkpointing (save & resume PPO agents)  
ğŸ“ Curriculum training with dynamic difficulty levels  
ğŸ—ºï¸ Dynamic map generation & multiple environment scenarios  
ğŸ¯ Multi-goal or hierarchical task setups  
ğŸª„ Real-time debugging overlays for policy behavior

